% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage[top=2.5cm,bottom=2.5cm,right=2.5cm,left=4cm]{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
 %\geometry{margin=0.5in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information
\usepackage{graphicx} % support the \includegraphics command and options
% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent
%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
%\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{times}
% These packages are all incorporated in the memoir class to one degree or another...
%\usepackage{biblatex} 
%\bibliography{Essay}
\usepackage{mdframed}
\usepackage{fixltx2e}
\usepackage{hyperref}
%pictures and figures
\usepackage{caption}
\usepackage{subcaption}

\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{lightpurple}{rgb}{0.8,0.8,1}
\lstset{frame=single,
  backgroundcolor=\color{lightpurple},
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\footnotesize\ttfamily},
  numbers=left,
  numberstyle=\tiny\color{black},
  keywordstyle=\color{blue},
  morekeywords={endif,bool},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}
%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...
%\usepackage{pdftex}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}

\begin{titlepage}
\begin{center}
\textsc{\LARGE University of Hertfordshire}\\[0.5cm]
\textsc{\Large School Of Computer Science}\\[1.5cm]
\textsc{\Large Modular BSc Honours in Computer Science}\\[1.5cm]
\textsc{\Large 6COM0282 â€“ Computer Science Project}\\[3cm]
\textsc{\Large Final Report}\\[0.5cm]
\textsc{\Large April 2013}\\[3.5cm]
\textsc{\Large A Comparison and Analysis of Multithreading Language and Library Implementations and Features with Respect to Conway's Game of Life}\\[0.5cm]
\vfill
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
E R \textsc{Michniak} \\ 10233252
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Colin \textsc{Egan} \\
\end{flushright}
\end{minipage}
\end{center}
\end{titlepage}

\pagebreak
\tableofcontents
\section{Abstract}
\section{Acknowledgements}
\section{Introduction}
With such a large number of choices open to programmers in the context of multi-core programming and writing concurrent code there is no wonder that it can be a bit of a minefield getting started. This investigation is aimed at providing an insight into the relevant features and facilities of popular (and not so popular) multi-threading solutions. Whilst also aiming to provide details of some of the quirks and eccentricities encountered during parallel program design and implementation.

The scope of this project includes a discussion of the procedures involved in designing and implementing parallel code, and the generic features of multi-thread capable libraries. A walk-through of the different code implementations and an analysis based on the {\bf core values} outlined later in this section. It is a concious decision to leave an in-depth discussion about different compilers and interpreters out of this report.
%Using Conway's Game of Life as a conduit to e set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , pexplore the facilities and functionality of different multi-threading libraries and languages, I will compare and contrast the different implementations I produce to investigate their overall appropriateness for simple parallel programs.
%Why Conway's Game of Life? 
\subsection{Motivation}
Designing parallel code, especially for a beginner, is an activity filled with difficult decisions about how to proceed. At every point in the development cycle the stakes are high, and the overall success of the application depends heavily on decisions made during the design and implementation stage. Most importantly, I believe the likelihood of producing a successful parallel application relies on the decision of what multi-threaded language or library to use. The motivation for this project resides my belief that there is a need for a concise resource that contrasts and compares the benefits and pitfalls of popular multi-threading packages for a parallel problem which is simple and common.
\subsection{Aim}
To compare and contrast multi-threading languages and libraries using Conway's Game of Life as an example of language facilities and features. The overall analysis of a particular implementation will be based around {\bf core values}. The successful satisfaction of these will lead to a conclusion based around the suitability of a particular model for a simple parallel problem. Whilst also conducting an investigation into the process of creating parallel code and the obstacles and challenges that come with it.
\subsection{Objectives}
\begin{itemize}
\item Provide a commentary on prototype and relevant research undertaken
\item Investigate and document the steps leading to the creation of a parallel algorithm.
\item Produce code implementations of the Game of Life in various multi-threading packages
\item Produce an analysis using quantitative and qualitative methods
\end{itemize}
\subsection{Core Values}
\begin{itemize}
\item {\bf Efficiency} A parallel program must run quickly and make good use of processing resources, that's the whole point! Efficiency will not only examine the speed of an implementation with respect to other implementations but also the overall speed-up, the overheads associated with using certain package directives to make code run correctly, the organisation of the code and how a different structure might perform more quickly or slowly, how compiler optimisation (if it's available) effects performance, and any other issues that present themselves.
%\subitem Concurrent applications must run quickly and make good use of processing resources. With regard to a concurrent algorithm, efficiency will examine the overhead computations that must be added to ensure a correct execution, how alternative arrangements of threads or organizations of tasks might work better or worse, and what other problems there could be with the performance of the threaded application.
\item {\bf Simplicity} A simple algorithm is one that is easy to develop, debug, verify, maintain and extend. This value will investigate how much code has had to be added to ensure proper parallel execution over the sequential version. Also, how much of the initial code structure remains? 
%\subitem The simpler a concurrent algorithm is, the easier it will be to develop, debug, verify, and maintain. In terms of concurrent code based on a serial version, discussions of simplicity will focus on how much extra code you have to add to achieve a concurrent solution and how much of the original structure of the serial algorithm remains.
\item {\bf Portability} How easy is it to move from one model to another? Using C code as a base, how hard is it to re-use existing algorithms and create a second implementation. Do the multi-threading directives translate easily? The degree to which a algorithm is ambiguous with another will describe how portable it is in that context.
%\subitem Portability discussions will examine the tradeoffs that could encountered if you used a different threading model from the implemented design. While this book is primarily dedicated to the design and exploration of multithreaded codes, one of the options discussed under portability will be distributed-memory variations of the algorithms.
\item {\bf Scalability} Technology is evolving, the number of processors in computers is only going to increase in the future. How scalable is the code produced? Does the addition of more threads provide a continual speed-up in line with the number of independent processors on a machine? If the size of the data set increases what's the overall effect on the programs execution? The degree to which an implementation can maintain a speed-up with the addition of increasing resources or workload will describe its scalability.
%\subitem Because the number of cores will only increase as time passes, your concurrent applications should be effective on a wide range of numbers of threads and cores, and sizes of data sets. Scalability refers to what you should expect with regard to how a given concurrent algorithm will behave with changes in the number of cores or size of data sets.
\end{itemize}
\subsection{Required Knowledge}
\subsection{Project Scope}
%Maybe just use the introduction to explain this?
\subsection{Project Structure}
The investigation begins with an in-depth discussion of cellular automata, in particular Conway's Game of Life. This is succeeded by a look at some of the main algorithms that make the game work, in sequential C. 
\subsection{Test Setup}
There is a significant amount of testing that will take place. Throughout the implementation and analysis stage, to test stability, efficiency and scalability.
\smallskip
\\My test platform:
\begin{itemize}
\item Intel Q6600 at 3.0GHz
\item 6GB of DDR2 at 800MHz
\item Linux Mint 14 x84-64
\end{itemize} 
Compilers, interpreters and runtime environments used:
\begin{itemize}
\item GCC - GNU C Compiler
\item G++ - GNU C++ Compiler
\item Javac - Java Compiler 
\item Python2.6 - Python interpreter
\item GCCGO - Go compiler with a GCC backend
\item Go - Go Compiler
\end{itemize}
\section{Background Research and Algorithm Design}
\subsection{Life}
Conway's Game of Life is an example of a {\bf cellular automaton} (CA). This is described \cite[Lesser, Wuensche, 1992 p6]{ref6} as a ``discrete dynamical system which evolves by the iteration of a simple deterministic rule''. When considering constructing a CA there are a few steps one has to follow:
\begin{itemize}
\item Firstly, consider that \emph{time is discrete} and progresses in steps, in this report I will mostly be referring to them as {\bf generations}. A term far more fitting for Conway's Game of Life. 
\item Secondly, an \emph{n-dimensional space} is divided and sectioned into {\bf cells}. This space will have a boundary condition associated with it. I will be implementing the space with an orthogonal toroidal array in 2-dimensions. This means that my boundary conditions would be {\bf cyclic} rather than {\bf absorbing}. {\it figure 1}
\begin{mdframed}
{\bf Extension Box:} However interesting and challenging it may be to implement the Game of Life in 3-D space it is not the focus of this project and for the sake of thread workload scheduling, clarity, and correctness of the result, 2-dimensions is safer and more predictable.back
\end{mdframed}
\item Thirdly, each cell has an {\bf attribute} from a limited set of attributes. In the case of Conway's Game of Life, each individual cell can either be dead or alive. Represented in most of my implementations by a 1 or 0. The combined values of all cells in the space are considered to represent the {\bf global state}.
\item Finally, to progress to the next time interval (for instance, to progress from t\textsubscript{0} to t\textsubscript{1}) a {\bf transitional function} is applied to each individual cell using its attribute and the attributes of its {\bf neighbourhood}. Such that, t\textsubscript{1} is the state of the automaton after having the function applied at state t\textsubscript{0}. In this context the Game of Life uses a \emph{Moore-neighbourhood} which consists of the 8 cells surrounding a central cell. However it is worth to note the existence of the \emph{Von Neumann neighbourhood} which pre-dates Moore's implementation. {\it figure 2}
\end{itemize}
\begin{figure}[h]
        \centering
        \begin{subfigure}[h]{0.3\textwidth}
                \centering
                \includegraphics[scale=1]{cyclic}
                \caption{Cyclic boundaries}
                \label{fig:cyclic}
        \end{subfigure}%
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[h]{0.3\textwidth}
                \centering
                \includegraphics[scale=1]{absorbing}
                \caption{Absorbing boundaries}
                \label{fig:absorb}
        \end{subfigure}
        \caption{Types of boundaries}\label{fig:boundaries}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[scale=0.75]{neighbourhood}
\caption{The two types of CA neighbourhood}
\label{fig:neighbourhood}
\end{figure}
Considered by many to be a \emph{zero-player game}, Conway's automaton was first revealed in a 1970 Scientific American article \cite[Gardner]{ref7} where his rules where enumerated and an explanation of how one might play his game was offered. This involved a very laborious method of using a checker board and some two-colour checkers (or something similar), to represent the {\bf global state} and every cells individual attribute. An 8 x 8 grid would require 64 applications of the transitional function to move forward just one generation! Maybe you can begin to see why a computational implementation would be useful?
\subsubsection*{The Laws of Life}
\begin{enumerate}
\item Survivals. Every live cell with two or three neighbouring live cells survives for the next generation.
\item Deaths. Each live cell with four or more neighbour live cells dies (is removed) from overpopulation. Every live cell with one neighbour or none dies from isolation.
\item Births. Each dead cell adjacent to exactly three neighbour live cells--no more, no fewer--is a birth cell. A life is placed on it at the next move. (i.e. the cells attribute is changed) \cite[Gardner, 1970]{ref7}
\end{enumerate}
These laws are going to form the basis of the {\bf transition function} which will be applied to the grid to progress to the next generation.\\
\\What makes the Game of Life so interesting and unique? In the book ``The Game of Life: Cellular Automata'' \cite[Bays, 2010, p1]{ref8} argues that it is ``...the discovery of ``oscillators'' (periodic forms) and ``gliders'' (translating oscillators).'' that are the source of its original fame and interest. These {\it lifeforms} represent a breakthrough in two-dimensional cellular automata and were discovered by mathematician William Gosper in 1970. A prize was offered by John Conway relating to his conjecture that there could be a configuration of the initial state that would constantly promote an ever increasing number of live cells in the global state. Gosper won the prize by demonstrating, and thus coining the term, a glider gun. {\it figure 3}\\
\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{gosper}
\caption{The Gosper Glider Gun}
\label{fig: Gosper}
\end{figure}
\\Why code? Even smallest grid requires a lot of time and effort to move from one generation to the next. Consider the configuration in the grid above and try to process a single generation using a manual method. Boring, repetitive and tedious isn't it? Thankfully computers excel at boring and repetitive tasks that humans find tedious. Also, it's interesting! There's no end to the different implementations that could be written to change some way in which the system executes. Finally, the most common implementation of the Game of Life would involve the allocation of some array structure, and considering the layout of computer memory this is very intuitive and easy to do.\\
\begin{mdframed}
{\bf Extension Box:} There is a vast number of different algorithms that could be implemented. For example, Hashlife, quad-trees, linked-list, bit-stream... An extension could be researched into the relative performance of different algorithms in specific languages and the language features which benefit or hinder execution. My pseudo-algorithm will be outlined later on however I will be focusing on realising and defining parallel points in the program execution and discussing the syntax, semantics and effectiveness for respective implementations. 
\end{mdframed} 
\subsection{Prototype Code, Parallelism Research and Algorithm Development}
A prototype sequential version of the Game of Life has been produced as part of my research into the algorithms available and memory structures I could have used. This version has been produced in C and can be broken down into a few abstract, yet simple, operations. This example is an illustration of one of the most basic of programming paradigms, {\bf sequential composition}. This is described by \cite[Chandry and Taylor, p68]{ref9} as when components are executed in order, one after the other. The composition terminates when the last component terminates. Considering components as individual sub-programs or functions in our C code. We can begin to see how the Game of Life may be executed sequentially. And quite slowly for that matter! Referencing {\it figure 4} we can see the {\bf sub-programs} or {\bf components} that make up the sequential algorithm for the Game of Life. I will talk about some of the more interesting and prominent snippets of code from this early implementation, however a full listing will be available, line-numbered and commented, in the appendix A.1.\\
\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{sequential0.png}
\caption{A context-level activity diagram of the Game of Life algorithm}
\label{fig: Algo0}
\end{figure}
\\This is the starting point for the entirety of the project. From this diagram I can get an overview of how one algorithm is different from another and construct new algorithms. By changing {\bf sub-programs}, by modifying {\bf program transitions}, and by delving deeper into {\bf components} and their constituents. The most important aspect of this diagram is going to be helping me realise and program the parallel regions of the automaton.
\subsubsection{Refactoring the Components}
{\it Figure 4} highlights that the first process in the Game of Life algorithm to take place is the allocating of memory for the array. Consistency across my code modules is paramount and {\bf DIM} references the chosen dimension of my game board. I need two identically sized blocks of memory to represent my two-dimensional grid. One for the current state, and one for the state of the next generation. As shown in {\it figure 4} at the end of an iteration of one generation the references to the grids are swapped, and the process repeats itself until {\bf MAXGEN} is reached. 
\begin{mdframed}
{\bf Knowledge Box:} The '{\bf \#}' symbol denotes a {\bf preprocessor macro}. I have used these extensively in my prototype code. It is a way of instructing the compiler, if certain conditions are met, which code to include or exclude from compilation. This is known as conditional compilation. It is especially useful, in the case highlighted below, when experimenting with different types as some blocks of code may stay the same however the prototypes or function declarations may change. Initially a {\bf macro} is defined at the top of the program and then tested in the main code block. This can be seen in appendix A.1 lines[12,13], lines[26-34], and lines[147-151].
\end{mdframed}
\begin{lstlisting}[language=C,caption={Array allocation using {\it malloc}, {\it calloc}, and {\it bool}}, morekeywords={malloc,calloc,bool}]
#if USEBOOL == 0
    #if MALLOC == 1      
        grid = (int **)malloc(sizeof(int *) * DIM + 2);
        new_grid = (int **)malloc(sizeof(int *) * DIM + 2);
        
        for (i = 0; i < DIM + 2; i++)
        {
            grid[i] = (int *)malloc(sizeof(int *) * DIM + 2);
            new_grid[i] = (int *)malloc(sizeof(int *) * DIM + 2);
        }            
    #else        
        grid = (int **)calloc(DIM + 2, sizeof(int *));
        new_grid = (int **)calloc(DIM + 2, sizeof(int *));
        
        for (i = 0; i < DIM + 2; i++)
        {
            grid[i] = (int *)calloc(DIM + 2, sizeof(int *));
            new_grid[i] = (int *)calloc(DIM + 2, sizeof(int *));
        }            
    #endif
#else
    grid = (bool **)calloc(DIM + 2, sizeof(bool *));
    new_grid = (bool **)calloc(DIM + 2, sizeof(bool *));
    
    for (i = 0; i < DIM + 2; i++)
    {
        grid[i] = (bool *)calloc(DIM + 2, sizeof(bool *));
        new_grid[i] = (bool *)calloc(DIM + 2, sizeof(bool *));
    }
#endif
\end{lstlisting}
In {\it listing 1} we begin to understand the plethora of options that are open to implementation in the considered context of {\bf array memory allocation} (We haven't even considered any of the other memory structures!). The general convention for all 3 blocks of code is the same, that we need to allocate a memory block for each of our pointers, {\it grid} and {\it new\_grid}, to address. The only things that change are the functions used for allocation, {\bf malloc} or {\bf calloc}, or the parameter passed to {\bf sizeof}.
\begin{itemize}
\item {\bf Malloc} will take one argument, the size in bytes of the memory to allocate. It returns a void pointer to the block of memory which is then cast to the type that we wish to use. In the case of line 3 it is the pointer to a pointer of int values. 
\begin{mdframed}
{\bf Example: }This chain of pointers is necessary as we need a two-dimensional array. Each index is calculated as an offset from the base address referenced by the pointer. For example grid[2] is 2 times the size of an int pointer away from the base address of the grid pointer. If {\it \&grid} is 0x10 then {\it \&grid[2]} will be 0x90, assuming a pointer size of 4 bytes.
\end{mdframed}
\item {\bf Calloc} will take two arguments, the number of array elements to allocate, and the size of each individual array element. This also returns a void pointer which has to be cast to a desired type. However, there is a difference in the behaviour of the function. Calloc will zero-initialise the memory allocated. Whereas malloc will not initialise any memory in the allocated block, the indeterminate values will remain. 
\item {\bf Bool} is a data type most Java programmers will be familiar with, however it was not included in the standard revision of C until the release of C99. To this day the {\bf bool} header file has to be included in the C program by a preprocessor macro as indicated by appendix A-1 line 6. 
\item {\bf Sizeof} is a function that returns the size of its parameter in bytes. Extremely useful in maintaining portability of C code across architectures. This is because the compiler will know what the size in bytes of an Integer is on its system...probably better than the programmer.
\end{itemize}
\begin{mdframed}
{\bf Knowledge Box:} Type-safety really takes the back-seat when programming in C! When we say something is {\bf type safe} we mean that the language guarantees that a value of one type can't be incorrectly used as if it were another type. This makes C very type {\bf unsafe}! Casting a pointer to another type allows the programmer to use it as that type even though the actual value could mean absolutely nothing. Also the function {\bf free} allows the programmer to deallocate memory and reassign a pointers and possibly use them later. The concept of {\bf casting} and {\bf dangling pointers} means that C can never be type safe! \cite[PLDI B, Lane]{ref11}
\end{mdframed}
\smallskip
After the array is allocated the next step in the Game of Life algorithm is to assign an initial global state to the grid by {\bf randomly spacing life forms in the array}. In the sequential program this is done by choosing a probability of life existing and randomly (not actually random at all - but that's another project!) placing live or dead cells. The function in {\it listing 2} iterates through the array using nested for-loops. Remember the allocation of the memory from the last code snippet? The parameters given to the malloc or calloc functions are DIM + 2 in size each time! This is because of my design decision to use a orthogonal toroidal array, which gives the grid cyclic boundary conditions. The grid has to have an extra 2 rows and an extra 2 columns to house the {\bf ghost cells} which will be explained shortly.
\begin{lstlisting}[language=C,caption={Randomly spacing lifeforms in the array}, morekeywords={malloc,calloc,bool}]
#if USEBOOL == 1
void fillRand(bool **grid_ptr)
#else
void fillRand(int **grid_ptr)
#endif
{
	int i, j;
	srand(SEED);

	cell_count = 0;
	life_count = 0;

	for (i = 1; i <= DIM; i++)
	{
		for (j = 1; j <= DIM; j++)
		{
			if (rand() % LIFE == 1)
			{
				life_count++;
				grid_ptr[i][j] = 1;
			} else {
				grid_ptr[i][j] = 0;
			}
			cell_count++;
		}
	}
}
\end{lstlisting}
However, in these loops we only care about the cells relating to the global state, so the iteration bounds are set at 1 and less than or equal to DIM respectively. Notice line 8, here the random function that's used on line 17 is seeded with a preprocessor defined value. This is to ensure that the random set returned will always be the same. Keeping the same input data and confirming by a replicable result is vital at this stage to confirm the correctness of the code. On line 17 the value returned by the {\it rand()} function is used as input to a modulus function with the macro {\it LIFE} which defines the probability of life occurring. In this case, if LIFE were to equal 3, then the approximate number of live cells would be equal to the total number of cells divided by 3. Also notice the preprocessor macro around the function declaration which will change the type of the parameter accepted.\\\\
\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{sequentialGhost.png}
\caption{A Level 1 Activity Diagram of the copying ghost cells process}
\label{fig: Algo1}
\end{figure}After the initial global state has been defined we can begin the main game loop! That brings us on to the {\bf copying of the ghost cells}. As mentioned before, cyclic boundary control means that the grid will perform as if it were laid out over a toroid. This allows such lifeforms as translating oscillators (like the glider mentioned in section 3.1) to continue infinitely as long as they are not obstructed by another live cell. At the beginning of each generation the columns at index [i][DIM + 1] and [i][0], and the rows at index [0][i] and [DIM + 1][i], are written to with the attributes stored in columns [i][1] and [i][DIM] and rows [DIM][i] and [1][i] respectively. Where i is any value between 1 and DIM for the first loop and 0 and DIM + 1 for the second. This is illustrated in an abstract manner in {\it figure 5} and in full in {\it listing 3} where lines 4 and 5, 10 and 11, represent the copying of the ghost cells for columns and rows respectively. This change in the range of iteration is so that we can ensure the cells in the corners have a value. A visual example of the memory elements and their values can be seen in {\it figure 1}. 
\begin{lstlisting}[caption = {Copying ghost cells}]
    /*copy ghost columns to grid */
    for (i = 1; i <= DIM; i++)
    {
        grid_ptr[i][DIM + 1] = grid_ptr[i][1];
        grid_ptr[i][0] = grid_ptr[i][DIM];
    }
    /*copy ghost rows to grid */
    for (i = 0; i <= DIM + 1; j++)
    {
        grid_ptr[0][i] = grid_ptr[DIM][i];
        grid_ptr[DIM + 1][i] = grid_ptr[1][i];
    }
\end{lstlisting}
Now that the initial grid is full to the brim, the program can begin processing the next generation by applying the transitional function for Conway's Game of Life. An abstract view of the algorithm is given by {\it figure 6} where the nested nature of the loops and the decisions based around the logic of the transitional function can be seen causing changes in the state of the array for the next generation. It should be fairly obvious to see how the rules listed in {\it figure 6} conform to the rules enumerated in section 3.1. This algorithm is written out in full in {\it listing 4} with the counting of the moore-neighbourhood taking place from line 12 and the application of the transitional function from line 18. Every rule must write something to the appropriate element of the array for the state of the next generation. this can be seen in line 20, a life being written, line 23, a death being written, or line 26, a survival being written. After this it is simply a case of swapping the grid and new\_grid pointers and incrementing the {\it gen} counter. The game will continue until {\it gen} equals MAXGEN. As mentioned earlier the correctness of my results is vital and a function to count the number of alive cells after the game has finished is also in the code base in A.1. This can be compared against other models to ensure correctness. 
\bigskip\bigskip\bigskip
%\begin{figure}[h]
\begin{lstlisting}[caption={Processing the Moore-neighbourhood and writing to the next generations global state array}]
#if USEBOOL == 1
void process(bool **grid_ptr, bool **new_grid_ptr)
#else
void process(int **grid_ptr, int **new_grid_ptr)
#endif
{
	int i, j, count;
	for (i = 1; i <= DIM; i++)
	{
		for (j = 1; j <= DIM; j++)
		{
			count =
			    grid_ptr[i - 1][j - 1] + grid_ptr[i - 1][j] +
			    grid_ptr[i - 1][j + 1] + grid_ptr[i][j - 1] +
			    grid_ptr[i][j + 1] + grid_ptr[i + 1][j - 1] +
			    grid_ptr[i + 1][j] + grid_ptr[i + 1][j + 1];

			if (count == 3 || (count == 2 && grid_ptr[i][j] == 1))
			{
				new_grid_ptr[i][j] = 1;
			} else if (count < 2 || count > 3)
			{
				new_grid_ptr[i][j] = 0;
			} else if (count == 2)
			{
				new_grid_ptr[i][j] = grid_ptr[i][j];
			}
		}
	}
}
\end{lstlisting}
%\end{figure}
\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{sequentialProcessGrid.png}
\caption{A Level 1 Activity Diagram of the application of the transitional function and rule logic}
\label{fig: Algo0}
\end{figure}
These blocks of code have helped me make some fairly crucial design decisions on my C and C++ versions and to a lesser extent my Java and Python versions. Earlier on I introduced the idea of a preprocessor macro and explained my use of them. This has helped me make two big design decisions. Firstly to use the function {\bf calloc} for allocating my array memory, and secondly to use the {\bf bool} import and data-type for the element attributes.
\begin{itemize}
\item {\bf Calloc} was chosen because, on a whim, I tried to validate the correctness of my code using two different programs for runtime analysis. {\bf Valgrind} and {\bf GProf}. Both programs reported runtime errors relating to the memory structures, during any type of access to my arrays. Evidence of the errors is in appendix A.3. Malloc seemed to be leaking and causing a large number of errors. One of which broke Linux indefinitely! After switching to calloc I had no such errors and the program ran correctly. Interestingly enough the results recorded from both the malloc version and the calloc version were the same the only difference was the memory leak. This error was still present even if the arrays were freed using the {\bf free(void*)} function.
\item The {\bf bool} library was chosen because, apart from being the obvious choice, in that each cell in the automaton has only two states, alive or dead; but because it was 3\% faster in testing, consistently. My initial logic was that fetching a boolean value from memory, with a size of 1 byte, compared to a 4-byte integer, means that the boolean saves on clock cycles as it uses less bandwidth (however this depends on the size of the buses involved in each area of the execution pipeline; I'm assuming a 64-bit bus width and word size considering my architecture). At least that is what I thought. I did some research into how the assembler treats the use of bool versus an int and I produced a piece of code that attempts to perform an if-statement on a bool and an int. Then made the compiler produce the assembly code for an optimised version(it's easier to read than unoptimised assembly). The code tells us that the only difference between both functions f() and g() is the {\it cmpl} and {\it cmpb} assembly respectively. There is no difference between the function of these calls other than one only expects a byte as an argument {\it cmpb}(compare byte) and the other is a simple logical compare. Knowing this I tried to find out what values were passed into the registers and in a x86-64 architecture the whole of the general purpose register has to be used so the boolean value will be padded to fit in the GPR. So where do the performance benefits come from? This can be seen in the appendix A.4.
\end{itemize}
\subsubsection{A Thought Process for Parallelism}
Why do we make code parallel? So we can increase the speed at which a process or program can execute, of course! There are three main levels of parallelism: {\bf instruction-level parallelism}, {\bf data parallelism}, and {\bf task parallelism}. Where instruction-level parallelism represents instruction pipelining, branch prediction, or super-scalar architectures \cite[Patterson, Hennessy, p41]{ref10}. Task parallelism is the simultaneous processing of different tasks (or processes). Data parallelism, the beast that I am concerned with, is the decomposition of data into smaller blocks so that work can be carried out on them in parallel by separate processing units \cite[Patterson, Hennessy, A-17]{ref10}.
\bigskip
\begin{mdframed}
{\bf Knowledge Box:} It is important to make a distinction between {\bf parallelism} and {\bf concurrency} even though it is common to use the two terms interchangeably. Concurrency is about dealing with many things at once, whereas parallelism is about executing two or more things at the same time. In concurrently structured programs the design is not automatically parallel. A concurrent task is one that is independent and {\bf concurrent decomposition} deals with breaking a program into pieces that can be executed independently.  Concurrently structured code can be parallel! Although if it is parallel it is a symptom of the environment and not necessarily the code produced. An example of this would be that single-core processors can run code in a concurrent manner, or in a way that may seem parallel even though it is not. \cite{ref12} Thus it is correct to say that parallelism is a subset of concurrency. 
\end{mdframed}
\bigskip
When we begin to think about realising and adapting the program to run in a parallel manner there are a few design processes and activities to consider before writing. {\bf Partitioning} involves dividing up the program into components that will allow parallelism then {\bf granularity} is adjusting the ratio of computation to communication for these parallel components and then {\bf mapping} this refactored design to computational units. \cite[p77,78]{ref9} A quantitative approach can be achieved by identifying code that {\bf can} be made parallel and code that {\bf can't} be made parallel. The application of this quantitative method is known as {\bf Amdahl's Law}. In the context of the prototype, a prediction on speed-up can be made my analysing the amount of time an entire run of the program spends in the cell processing phase and the amount of time the program spends executing code that can only ever be sequential.
\bigskip
\begin{mdframed}
{\bf Knowledge Box:} Amdahl's Law states that the performance enhancement possible with a given improvement (for instance: creating a parallel region from a serial region) is limited by the amount that the improved feature is used. \cite[Patterson, Hennessey, p51]{ref10}\\ The formula for calculating a speed-up S for N processors can be explained by:\bigskip

\centerline{$ S(N) = \frac{1}{(1-P) + \frac{P}{N}} $}
\smallskip
Where {\bf P} is the proportion of code that is subject to improvement and can be made parallel.
\end{mdframed}
\bigskip
An analysis of the sequential code using the GNU runtime profiler {\bf Gprof}, shows the amount of time the program spent executing in each function. The call-graph can be a little hard to look at and interpret so I have shown the flat profile in {\it figure 7}. It should be noted that the code has been refactored so that each function represents only one task. This, whilst also being good programming practice, is necessary for Gprof to give a more detailed output. A full listing of the sequential code can be found in appendix A.1 and the full Gprof output in appendix A.2. The function {\bf process()} will produce the function calls {\bf getCount()} and {\bf applyRule()}. The parameters for this execution of the simulation are 100 generations with a 1024 square grid. This can be observed in the {\bf calls} column where the process function has 100 calls (1 per generation) and both getCount and applyRule have 104857600 calls; 1048576 cells in the grid, 100 times. Gprof reported the time of execution at 4.51 seconds whilst the timing feature imported from {\it lrt} (or libRealTime) reported 4.967. This disparity is normal when profiling code and luckily enough the difference isn't drastic enough to warrant any concern.\\
\begin{figure}[h]
\caption{Gprof Flat Profile from seqGoL.c}
\begin{verbatim}Each sample counts as 0.01 seconds.

  %   cumulative   self                self     total           
 time   seconds   seconds    calls    ms/call  ms/call  name    

 56.47      1.98     1.98 104857600    0.00     0.00    getCount
 29.95      3.02     1.05 104857600    0.00     0.00    applyRule
 12.90      3.48     0.45      100     4.51    34.76    process
  0.57      3.50     0.02        1    20.06    20.06    fillRand
  0.29      3.51     0.01        2     5.02     5.02    printGrid
  0.00      3.51     0.00      100     0.00     0.00    copyGhostCells
  0.00      3.51     0.00        1     0.00     0.00    init
\end{verbatim}
\end{figure}

From this data an attempt can be made to calculate the expected speed-up for different numbers of processors. Using the values in the {\bf \% time} column of the table one can estimate a value for {\bf P} the proportion of code that can be made parallel. However, it should be noted that when converting a sequential algorithm to a parallel one it is normal for the {\bf problem size} to increase. In this context, this means the addition of scheduling and synchronisation algorithms. This will be covered later in more detail in section 4.1.
\subsubsection*{Partitioning}
The main concerns here are the ability to maintain {\bf scalability} and {\bf hide latency} when the program is run in a parallel manner. Where scalability refers to the measure of increased performance for an increased number of computational units. And hiding latency is the extent to which the overheads created by an increase of communication and code are hidden by an overall speed-up and increased processing power (by utilising multi-core or multi-computer architectures). \cite[p78,79]{ref9} To address the issues of scalability and latency the initial workload will be {\bf decomposed} into a number of partitions less than or equal to the number of computational units available.\\
%something is wrong with the formatting if this line is removed?
\begin{mdframed}
{\bf Example Box:} When talking about hiding latency the concept usually refers to the down-time a processing unit may encounter when it has finished its work or is waiting for information. This is talked about in a more relevant context later in the next section. Here, however, I am referencing the overheads created by that actual inclusion of code that makes parallelism possible. For example, the calculation of the amount of work to schedule each processing unit and the time spent waiting for synchronisation increase overall program latency as it has the possibility of increasing the down-time of a thread or processing unit.
\end{mdframed}
\bigskip
How do we begin to partition the problem into parallel code? The answer is {\bf domain decomposition}! This involves dividing up the data of the program so that it can be executed in a parallel manner. My most prominent data structure is the set of two-dimensional arrays, one that is read from by the program and one that is written to. So, in the context of Conway's Game of Life, this would mean dividing up the array representing the global state into smaller arrays or {\bf chunks} to signify the work for each individual processing unit to compute. In this case the transitional function could be applied by independent computational units on their respective workloads which have been calculated by decomposing the data domain. However, this highlights the issue of {\bf problem scaling} where the number and size of the chunks created during the partitioning process should be proportionate to the amount of available processing units. This is so to maintain and maximise the performance improvements from parallelising the work. This can be seen in {\it figure 8} where an eighteen-by-eighteen grid representing the global state and ghost cells is decomposed to show the work taken out by individual computational units.
\bigskip
\begin{mdframed}
{\bf Knowledge Box:} As well as {\bf domain decomposition} there exists concepts of {\bf functional decomposition} and {\bf irregular problem decomposition}. Functional decomposition involves decomposing the function of the problem as opposed to the data. Irregular problem decomposition refers to when the program structure cannot be determined before runtime. This could be caused by a dependency to input data.\cite[p85]{ref9}
\end{mdframed}
\begin{figure}[h]
\centering
\includegraphics[scale=0.7]{DomainDecomposition.png}
\caption{The Global state and how it might be partitioned to support parallel processing}
\label{fig: Para1}
\end{figure}
\subsection*{Granularity}
Granularity is about adjusting the ratio of {\bf communication} to {\bf computation} and it heavily influences the choice of data structure for a parallel implementation. Increasing the locality of the data structures increases the ratio of computation to communication. In a less abstract sense, by separating the input and output data arrays more computation can be completed as less communication is needed by the computational units to arrange synchronisation. This is achieved through using two arrays; one to hold the current global state and one to hold the future global state. The trade off in increased memory usage is worth it considering the speed up.\cite[p87]{ref9} The current approach focuses on a {\bf coarse-grained} structure. Whereby the data sets worked on by each thread are as large as possible and are available through a shared resource that requires little to no communication to access. Imagine if the grid were divided into smaller chunks and each thread handled more of these smaller pieces. Whilst increasing the flexibility of the data processed we have also increased the overheads associated with processing. This {\bf fine-grained} solution will require more synchronisation between threads (imagine the recombining of the processed chunks) and ultimately be slower. In the context of Conway's Game of Life this issue is rarely visible; the components are decomposed to remain independent and so it is wise to process as much as possible to reduce communication and increase computation.
\subsection*{Mapping}
Mapping is the process of assigning components to computational units for execution. It is the final step in creating the parallel algorithm and it brings together all of the processes completed so far. There are many different methods for successfully mapping and I am going to talk about, and ultimately implement, a method of mapping known as {\bf indexing}.

Indexing is most commonly used in domain and functional decompositions and makes use of indexes obtained from the partitioning process to assign work to processing units. In the context of Conway's Game of Life, {\it figure 8} describes how the global state array may be partitioned to allow parallel processing. If each of these partitions is assigned a number representing the index of the thread which is going to be dealing with it then an algorithm can be deduced which begins to describe different work allocations. This is described in {\it listing 5} and written in pseudo code. The {\bf start} and {\bf stop} values refer to global state array indices, the {\bf id} value to the Identifier or index of the computational unit, the thread to which the work is assigned, and {\bf numberOfThreads} the desired level of partitioning and number of computational units to define.
\begin{lstlisting}[language=C, caption={Pseudo Domain Decomposition and Thread Mapping Algorithm}]
for id = 0; id < numberOfThreads; id++
    start = ((dimension / numberOfThreads) * id) + 1
    stop = (dimension / numberOfThreads) + start - 1
    createThread(id, start, stop)
\end{lstlisting}
The algorithm in {\it listing 5}, when implemented, will provide a facility for decomposing the global state into {\bf partitions} that are assigned {\bf indexes} which are {\bf mapped} to their appropriate computational unit. Using this information, along with the previous version of the activity diagram for the sequential algorithm, it is possible to redefine the diagram with a control flow that resembles the complete Game of Life parallel algorithm. This is seen in {\it figure 9}. Notice the inclusion of a new process {\bf Schedule work based on number of threads}, this refers to the code in {\it listing 5}. Also visible are two black bars referred to as {\bf fork} and {\bf join}, these keywords and the symbols shown here refer to the synchronisation that has to take place to ensure the algorithm runs without fault and will be explained in full in the next section.
\smallskip
\begin{mdframed}
{\bf Extension Box:} It is worth noting that the described method of work allocation is known as {\bf static}. In that, the allocation of work to the threads does not change after its initial assignment. There is a concept of {\bf dynamic} work allocation that could also be implemented by the simulation. This could involve an implementation of a work stealing algorithm whereby, if a thread has finished its initially allocated work load, it could steal work off another thread that is still executing. In addition to this a dynamic work allocation algorithm would involve the adoption of a {\bf fine-grained} data structure and along with it the overheads of increased communication. As interesting and challenging as this would be I feel it is out of scope of the project as my main goal is to compare and contrast the implementations of different multi-threading languages and libraries.
\end{mdframed}
\begin{figure}[h]
\caption{A Level 0 Activity Diagram of the parallel Game of Life Algorithm}
\centering
\includegraphics[scale=0.45]{parallel0.png}
\end{figure}
\subsection{Threads}
%Do I need to say this?
This section will focus on the methods and implications of using threads. Nevertheless I believe a definition is necessary for context. A {\bf thread} is a concurrently executable process unit with access to a {\bf shared memory} space. Being a {\it concurrent} component means that a thread is basically a sequential program by itself. A thread runs by calling functions from code that exists in its shared memory space. Threads are {\bf asynchronous} they run whenever they can, at different speeds, and at different times. They may be delayed, lock or even destroyed for reasons which can be hard to realise. \cite[p70]{ref13} 

\subsubsection{Thread Creation}
One of the aims of this investigation is to look at different types of threading solutions available at the moment. How could I begin any way other than to show a ``Hello World'' example, with a twist of course. For this small introduction I won't be talking about any thread package in a great detail, this is a focus on the different code structures that can arise from different threading strategies. {\it Listing 6} shows a simple C program, this much should be obvious, however there is the addition of 4 lines and a single block encased in curly braces which are providing the basic facility for threading. The first point of interest is line 1, this statement includes the header file for the OpenMP package. A very powerful {\bf implicit} threading API. Line 7 defines two variables one for storing the identifier of any created threads and {\it nthreads} that stores an integer referring to the number of threads for OpenMP to create. Line 8 is a compiler specific directive that indicates the fork point for the OpenMP package. At this point the OpenMP API will create a number of threads indicated by the {\it nthreads} integer, and each thread will have a private field {\it th\_id}. The successful execution of this directive now means that all the code in between the curly braces on lines 9 and 12 will be in a OpenMP parallel region. At this point {\it nthreads} are created and each one attempts to execute the code in the parallel region. The private field {\it th\_id} is used on line 10 where the OpenMP function {\bf omp\_get\_thread\_num()} assigns the threads identifier to the private field. This can be useful to create sequential regions inside parallel ones, however more on this later. The ultimate conclusion of this code is that each created thread executes the {\bf printf()} function on line 11. There you have it, 4 threads, 4 lines of output. Easy, right?
\begin{lstlisting}[language=C, caption={Hello World from OpenMP}]
#include <omp.h>
#include <stdio.h>
#include <stdlib.h>
 
int main (int argc, char *argv[])
{
  int th_id, nthreads = 4;
  #pragma omp parallel private(th_id) num_threads(nthreads)
  {
    th_id = omp_get_thread_num();
    printf("Hello World from OMPthread %d\\n", th_id);
  }
  return EXIT_SUCCESS;
}
\end{lstlisting}
\begin{verbatim}
Hello World from OMPthread 0
Hello World from OMPthread 2
Hello World from OMPthread 1
Hello World from OMPthread 3
\end{verbatim}
\subsubsection*{Explicit and Implicit Threading}
That example may seem unnecessarily trivial however there is method in my madness. The explained code in {\it listing 6} is an example of a package which employs an {\bf implicit} threading model. This means that a lot of the creation and management of the threads (even sometimes the scheduling!) is hidden from the programmer to make the entire job a lot easier. The {\bf \#pragma} statement in {\it listing 6}, line 8 is a directive for the compiler to {\bf fork} a {\bf team} of threads. At the end of the parallel region, when all of the statements inside the curly braces have been executed by each individual thread, the team will {\bf join}. This pattern could be repeated over and over again in a single program. This style of parallel processing is known as the {\bf fork-join} model. This makes creating parallel regions extremely simple and, implemented in C, the performance is still better than most. However with increased simplicity the package loses its flexibility and expressiveness.\cite[p83]{ref14} The non-transparent nature of some of the directives in implicit threading packages create debugging extremely difficult; a situation could arise where the programmer simply doesn't know what is happening inside the parallel region!

So, we want more control? {\bf Explicit} threading is the answer! It enables the programmer to control all aspects of the threads they intend to create. From creating a thread, assigning a function to it, synchronising a team, or controlling access to resources. {\it Listing 7} is an example of a explicit threading library, PThreads. It should be immediately apparent that the layout of this code is extremely different to that of the OpenMP example. First the programmer must create a field of type {\bf pthread\_t} this is the threads container (often known as the thread's {\bf handle}) and is used to create the thread, associate it with a function and refer to it in the main context of the program. The actual thread will begin running as soon as it is created by the {\bf pthread\_create()} function. This expects arguments referrring to the handle to initialise, optional thread attributes, the function to be assigned to the thread and any additional arguments in the form of void pointers. Already this is more complex than the implicit thread example yet the level of control the programmer has over threads and what they execute is far greater.  \cite[p89]{ref14} In the {\bf for} loop on line 19 each one of the pthread handles in the {\it threads} array are created and associated to the same function, PrintHello. The only thing that changes is the argument, a void pointer to an integer referencing the identifier of the thread. Unlike OpenMP this is not explicitly defined by the API, in PThreads an identifier exists only if the programmer wishes it to. After this point the team of threads each execute the code inside the PrintHello function and are destroyed by the {\it pthread\_exit()} function. 

The obvious trade-off for increased flexibility is the increased complexity of the code. Also, a big difference to note between both models is the range of functionalities offered. You may notice that unlike the implicit example, there is no facility in explicit threading libraries to simply {\it create} a parallel region. Functions are assigned, parameters are passed, in this maelstrom of handles, threads, parallel regions and functions how is everything synchronised? How can everything possibly run smoothly? 
%delegation and inheritance. OOP vs. Sequential. 
\begin{lstlisting}[language=C, caption={Hello World from PThreads}]
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>
#define NUM_THREADS	4

void *PrintHello(void *threadid)
{
   long tid;
   tid = (long)threadid;
   printf("Hello World from pthread %ld\n", tid);
   pthread_exit(NULL);
}

int main(int argc, char *argv[])
{
   pthread_t threads[NUM_THREADS];
   long t;
   for(t=0;t<NUM_THREADS;t++)
   {
     pthread_create(&threads[t], NULL, PrintHello, (void *)t);
   }
   
   pthread_exit(NULL);
}
\end{lstlisting}
\begin{verbatim}
Hello World from pthread 0
Hello World from pthread 1
Hello World from pthread 2
Hello World from pthread 3
\end{verbatim}
\subsubsection{Synchronisation}
This is defined as the art of coordinating simultaneous threads of execution. At its simplest it involves gathering a team of threads at a point in the program before they are allowed to proceed. This practice is known as a {\bf join} function. It can be seen as the opposite of a {\bf fork} operation. If fork creates a team of threads then join brings them back together before terminating. This is extremely useful if at any step during the execution there is a section of the parallel algorithm that requires a task to have been completed, or a task that only needs to be done once, a bottleneck. The concept of a {\bf barrier} also exists and is like a join inside a parallel region, except the threads aren't terminated when the team is synchronised. At a barrier each thread that encounters it waits until all the threads in the team arrive. Upon arrival of the last thread, all threads are released and continue execution. \cite[p265]{ref14}

The other most basic type of synchronisation is when a thread demands use of a shared resource. It would create havoc if every active thread could access any given shared resource at any time. Consider the scenario of a simple function that increments an integer, a shared resource. Thread 1 reads the integer, thread 2 reads the integer, thread 1 increments the integer, thread 2 increments the integer. Something is wrong here. If a function that increments a shared resource is called twice, the expected answer would be two more than its initial value. However in this example the value is actually only one more than the initial value. This is called a {\bf race condition}. Both threads had access to the resource at the same time and before thread 1 could write back its answer, thread 2 had already read the initial value. A simple example, yet on a larger scale this is a very real danger. The solution? {\bf Mutual Exclusion} of a shared resource. The process of using a {\bf mutex}. \cite[p272]{ref14} A more concise definition is that this would occur when one thread reads from a location that is also being written to by another thread. By using mutual exclusion the programmer can control access to a function so that only one thread can be executing the code at any one time. \cite[p18]{ref14}
%explain join
%join = barrier?
\subsubsection{Getting into Trouble and the Game of Life}
In the context of the parallel Game of Life algorithm it is worth noting that there are specific points in the code that require thread synchronisation to ensure the correctness of the result. Referring to {\it figure 9} there are two synchronisation points. The threads must all {\bf fork} after the ghost cells have been correctly copied and {\bf join} before the grid pointers are swapped and the main game loop iterates. Throughout the code this effect will be achieved in different ways and discussed at length in terms of the differences and solutions for each individual implementations

What would happen if we neglected to synchronise? Well, different things. Firstly both the copying of the ghost cells and the swapping of the pointers should only occur once per cycle. Secondly, these activities are essentially writing to the contents of the array whilst the other threads are waiting to read from it. Without synchronising the threads a {\bf race condition could} occur. A race condition, outlined in the earlier section, is about contention of resources. Where, in a team of threads, one is trying to write, whilst others may be trying to read. The outcome of the read value can never be guaranteed without using synchronisation. In the Game of Life parallel algorithm, this is achieved by using {\bf barrier} objects to alter the flow of control in the algorithm it is explained in-depth in the OpenMP implementation.

Another possible consequence of not synchronising is the {\bf consumer-producer} problem. However, during the design stage of the Game of Life parallel algorithm this outcome was made impossible by the choice of data structure. Nevertheless I will explain it with a relevant example; Imagine a dynamic workload scheduling structure for the Game of Life's parallel algorithm.\cite[p10]{ref13}
\smallskip
\begin{mdframed}
{\bf Note Box: } This technique could be described as fine-grained. As the size of an individual datum that could be mapped to a computational unit is the smallest possible. One cell. As discussed earlier, the flexibility of the algorithm has increased. Also, this implementation would make a great starting point to implement a work stealing version of the algorithm. However, the algorithm now has more significant overheads involved with scheduling and synchronising the work. Is it worth it?
\end{mdframed}
\smallskip
{\it X} threads working on calculating the moore-neighbourhood where there is one producer and more than one consumer. The consumer waits for data to be put in its queue by the producer.  The producer waits for consumers to have room in their data structures for its data. The producer in this example would be the main thread and could operate on a system where, as soon as relevant cells have been copied during the ghost cell phase, begin dispatching work.
\begin{lstlisting}[language=C, caption={Dynamic Work allocation pseudo-code for the Game of Life}]
struct queueItem
{
    int x, int y;
}


calcMoore(queueItem *, grid *, new\_grid *)
{
    //read neighbour data from grid pointer
    //apply rule and write result to new\_grid pointer
}

consume()
{
    if(queue.size() == 0)
    {
        wait();
    }
    
    calcMoore(queue.getTop())
    
    queue.pop()
}

produce()
{
    if(queue.size() == MAXSIZE)
    {
        wait();
    }
    queue.push(queueItem *);
}
\end{lstlisting}
The consumer/producer scenario requires a lot of synchronisation to run smoothly and there are many issues that can arise from using it. Consider line 15 and line 27 of {\it listing 8}. What if the consumer makes the call to check the size of the queue, but before the comparison can be made the producer has put work in the queue? The program has just generated a race condition. The consumer thread will continue to wait and, because it hasn't realised the work in its queue, will remain in wait. In the meantime the producer thread will continue to add items to the consumer queue until it is full and then end up in a state of wait as well. The solution to this would be to use mutual exclusion on any thread requesting the size of the queue. However this still isn't enough, if a thread does end up in waiting then the other must communicate to it via a {\bf notify()} function, or similar, that it should wake up and check its queue for work.\cite[p18]{ref14}

The downside of using synchronisation objects is the overheads created by their use. Extra memory space allocated for locks, extra functions calls for use and destruction, extra time acquiring and releasing locks. Not only that but if implemented incorrectly they can cause blocks and locks of their own. \cite[Granularity]{ref15}

%\item Von Neumann architecture
%\subitem Von Neumann bottleneck?
%\item Harvard Architecture
%\subitem differences? Bottlnecks?
%\item Sequential vs. Threaded
%\item Common issues: Consumer-Producer, Cooperation, Race condition, dead lock, live lock.
%\item What would happen if we didn't have the synchronisation directives in the code?
%\item System kernel? How this effects the handling of threads.
%\item Introduction to packages/languages that will be used.
%\subitem Briefly explain the differences, look (very quickly) at some syntax.
%\item sleep why is this bad?
%\item thread states. ready. executing. dead. etc... HOW THE SYSTEM HANDLES THEM!
%\subitem how does (generically) a language interface with a systems threads?
%\subitem how does join work?!
%\item more needs to be here!

%\section{Languages, Compilers, and Interpreters}


%skip this!?!?!?!?! DEFINITELY!

%\item OpenMP and Boost and the appropriateness for different programming paradigms. Look into c++11!!!
%\item Interesting point: TeX the package used to create documents is a form of compilation.
%\item Introduce the languages more formally.
%\subitem Is history necessary here? Java history is quite interesting. 
%\subitem C99 not supported in windows. Worth mentioning?
%\subitem Language type (Functional, procedural/imperative, OO)
%\subitem Language features (typing, inheritance, classes, shared objects, evaluation, verbosity)
%\item Explain the difference between a compiler and an interpreter (don't forget JIT). 
%\subitem Maybe do this during the language introduction? 
%\subitem Highlight which language uses which.
%\subitem Mention optimisation, take a quick look at some of the C features that offer this (even the deprecated ones: register etc...)
%\subitem how do interpreters handle optimisation?
%\subitem how can they ever keep up with compiled languages?
%\item Recommendations based on research
%\subitem Is there one language that offers everything?
%\subitem Could a desirable feature be made available to another language/library at little cost?
%\item Requirements elicitation? (It has to go somewhere!)

\section{Implementation}
The languages I'm going to look at have been picked to highlight the fundamental differences in parallel design and threading library functionality and facilities. 
\begin{itemize}
\item {\bf OpenMP} - A mixture of an implicit threading package with an extremely fast and efficient language that also houses an {\bf optimising compiler}.
\item {\bf Java} - An Object-Oriented language and a threading package that supports explicit threading via the inheritance and implementation of a thread class.
\item {\bf C++ 11} - A relatively new addition to the C++ standard library. It is an explicit threading model that supports threading by function delegation.
\item {\bf C++ Boost} - The original alternative to using PThreads in C++. This explicit threading solution will be different from the C++11 implementation by the use of a thread class system similar to Java.
\item {\bf Go and Goroutines} - A recently released language that is created to perform the functions of a systems language only with a more flexible type system. The system of creating parallel code is also extremely different from anything encountered in other models.
\end{itemize}
\begin{mdframed}
{\bf Knowledge Box: } Whilst it may be that compilers and interpreters are not the focus of this report. It is important to define and distinguish between the various mechanisms used in the languages that are used to provide implementations. A {\bf compiler} is . An {\bf interpreter} is . An {\bf optimising compiler} is .
\end{mdframed}
\bigskip
As well as these 5 implementations I have included a Python implementation. This language is {\bf dynamically typed}, {\bf interpreted}, and supports a wide-range of programming styles. In particular interest to me, was the support for functional style {\bf list comprehensions}. This functionality will be used to provide a Game of Life implementation where the programming style is different to the others that have been proposed. List comprehensions are powerful constructs which are unique in their likeness to {\bf set-builder notation} in {\bf set theory}. One property of the functional program paradigm is that they can be proven mathematically. My idea being that if I can create an implementation in Python using set-builder notation and then list comprehension I can circumvent the process of verification of parallel algorithms or formal proofs. Two subjects which have enough content for an entire project each.
%\subitem class diagram for OOP models
%\item Validation technique

\subsection{C with OpenMP}
This is the obvious first choice for parallel implementation as the design of the parallel algorithm was based around the sequential prototype which was produced in C. Being such a flexible and low-level systems language it gave me an opportunity to look into some features that just aren't present in more modern languages. These are talked about in the next subsection.

C allows the use of preprocessor macro statements. At the top of the file is a list of 10 or so items which represent the settings for the {\bf conditional compilation} of the code({\it listing 9}). This is not only good practice, as it makes any changeable settings very visible to the programmer, it also makes the object file smaller and useless code that may be spent dissecting command line arguments is not present. 
\begin{lstlisting}[language=C, caption={OpenMP program settings in file}]
#define MAXGEN 10000       //the number of generations to compite
#define DIM 512            //the dimension of the grid 
#define LIFE 3             //probability of life, only used if no file read
#define BOOL 1             //use bool for the data type of the arrays, else int
#define SEED 2012          //seed for random algorithm, only used if no file read
#define THREADS 4          //number of threads to allocate
#define SHARED 1           //use shared memory or private memory for threads
#define INLINE 1           //use inline function prototypes
#define REGISTER 1         //use register keyword for global array definition
#define FILENAME "512.dat" //the filename to read in
\end{lstlisting}
These settings are then used to conditionally declare the function prototypes and global variables for the program shown in A.2 - lines 27-60. In the same way, conditional compilation is used to create the correct data structure for the global state arrays shown in A.2 - lines 89 - 107.

The structure and scope of the global state arrays is worth noting. Whilst the variable for these arrays is in the global scope, all of the code that accesses them does so through a pair of copied pointers; {\bf new\_grid\_ptr}, and {\bf grid\_ptr}. Defined in A.2 - lines 66 - 70 and used in A.2 - lines 110, 111. This is to simplify the process of swapping the grids over after the processing of a generation, see {\it figure 9} for reference. Then A.2 - line 116 performs a call to the readFile() function. It can be seen in A.2 - lines 399 - 442. 

A.2 - line 119, declares the private integers for the OpenMP {\bf parallel region}; {\bf start}, {\bf stop} and {\bf tid}. This is the static work allocation calculation algorithm, shown in {\it listing 10} lines 17 and 18. It should be noted that this is based off the pseudo-code algorithm in {\it listing 5}. However the real line of interest is line 11. This is the OpenMP pragrma directive which will direct the compiler to create a team of threads. There are two constructs which are applicable to this problem; {\bf omp parallel} and {\bf omp for}. The construct simply creates a team of threads with parameters defined by the list of {\bf clauses} outlined by the programmer. Clauses help the programmer gain a bit of control over how the OpenMP API manages the implicit threads. Used in this example I am defining some {\bf shared} and {\bf private} variables for each thread. The shared clause means each thread has access to a copy of the listed value in shared memory space. In this case the {\bf grid\_ptr}, {\bf new\_grid\_ptr}, {\bf temp\_ptr}, and the number of threads. The private clause means that each thread has its own {\bf local} copy of the variable listed. The {\bf gen} variable is used as a loop counter, the {\bf tid} variable is the thread's identifer, the {\bf start} and {\bf stop} variables define the limits of the static work allocation. After the {\bf \#pragma omp parallel} directive every statement inside the {\bf parallel region}, defined by the curly braces, is executed by each thread in parallel. 
\begin{lstlisting}[language=C, caption={OpenMP directive use and thread creation}]
//......

/*private OMP vars */
int start, stop, tid;

/**
 * Create a team of threads (defined by nthreads) to work on the following code.
 * Run for MAXGEN iterations of Game.
 **/
#if SHARED == 1
#pragma omp parallel shared(grid_ptr, new_grid_ptr, temp_ptr, nthreads) private(gen, tid, start, stop)  num_threads(nthreads)
{
	/*assign the thread an identifier*/
    tid = omp_get_thread_num ();

    /*define the amount of work for each thread */
    start = ((DIM / nthreads) * tid) + 1;
    stop = (DIM / nthreads) + start - 1;
    //....
\end{lstlisting}
The {\bf for} construct differs from {\bf parallel} by also implicitly managing the scheduling of the work. Defined as a {\bf work-sharing} construct, this could replace the use of the parallel region by being placed directly before the main game for loop and it would implicitly share the loop iterations between the created threads. Replacing the need for any for static work allocation to take place. However, in practice this was not so, I could never guarantee the correctness of the results. A symptom of implicit threading is that they are notoriously hard to debug. Therefore I opted for the construct that gave me the most control: parallel. {\it Listing 11} shows the main game loop which is {\bf inside} the parallel region defined by the OpenMP directive. 
\begin{lstlisting}[language=C, caption={OpenMP Game of Life main game loop}]
//...
for (gen = 0; gen < MAXGEN; gen++)
{
    #pragma omp single
    {
        copyGhostCells(grid_ptr);
    }
            
    process (grid_ptr, new_grid_ptr, start, stop, tid);

    #pragma omp barrier
            
    #pragma omp single
    {
        temp_ptr = grid_ptr;
        grid_ptr = new_grid_ptr;
        new_grid_ptr = temp_ptr;
    }
}
\end{lstlisting}
On line 4 the {\bf single} directive means that the first thread to reach the block will execute the statements inside and it will only happen once. There is an implied {\bf barrier} at the end of the block which requires all the other threads to gather there. This allows the completion of the {\bf copyGhostCells} function before the team moves on. Line 9 is the main game function {\bf process()} this is seen in {\it listing 12}. The similarities between this version and the sequential version in {\it listing 4} should be visible with the exception of the conditional statement in the for loop on line 5. This is the last factor in ensuring the threads only process the work that was allocated to them. Each thread will read from the array pointed to by the {\bf grid\_ptr} and write to the array pointed to by the {\bf new\_grid\_ptr}. This ensures that the algorithm will not have to use locks or synchronisation to control the write/read operations on these data structures.
\begin{lstlisting}[language=C, caption={OpenMP Process chunk and calculate moore-neighbourhood}]
inline void process (bool ** grid_ptr, bool ** new_grid_ptr, int start, int stop, int tid)
{
  int i, j, count;

    for (i = start; i <= stop; i++)
    {
        for (j = 1; j <= DIM; j++)
        {
            count =
            grid_ptr[i - 1][j - 1] + grid_ptr[i - 1][j] +
            grid_ptr[i - 1][j + 1] + grid_ptr[i][j - 1] +
            grid_ptr[i][j + 1] + grid_ptr[i + 1][j - 1] +
            grid_ptr[i + 1][j] + grid_ptr[i + 1][j + 1];

            if (count == 3)
            {
                new_grid_ptr[i][j] = 1;
            }
            else if (count < 2 || count > 3)
            {
                new_grid_ptr[i][j] = 0;
            }
            else if (count == 2)
            {
                new_grid_ptr[i][j] = grid_ptr[i][j];
            }
        }
    }
\end{lstlisting}
The barrier in {\it listing 11} on line 11 ensures that all threads gather at this point. This means each thread will have finished processing their respective chunks and the pointers can be swapped to allow the generation to complete. If the synchronisation clause was not here then the program runs the risk of swapping the pointers before the process of the global state has completed. A {\bf race condition} would occur and a thread still in the processing phase would write to the wrong location. The final OpenMP directive is another single block. The first thread to reach this block is responsible for swapping the pointers and it will only happen once. The threads gather at the implied barrier and the iteration continues to the next generation.
\bigskip
\begin{mdframed}
{\bf Extension box: } I have also implemented a version of the Game of Life using thread {\bf local memory} as opposed to {\bf shared memory}. In this version, after the static work allocation algorithm, each thread is allocated a grid which is stored in its local memory space. This grid is the exact size of the chunk that it needs to process and the chunk from the global state is copied into this local memory array. The idea being that this could be implemented on a distributed parallel processing architecture. Using separate CPUs that only communicate to unzip or zip up the global state arrays. However, the code on a multi-core platform, performs slower than the shared memory version which is not surprising. I feel that it is a brilliant example of {\bf coarse-grained} versus {\bf fine-grained} data structures and whilst I will not talk about it directly in the report my code is in the appendices A.2 lines 169 - 227, lines 375 - 394.
\end{mdframed}
\subsubsection{Optimising Compiler Features}
What exactly {\bf is} an optimising compiler? C, C++, and Go have forms of optimising compilers. All based around {\bf GCC}, the GNU C Compiler. In a very general sense an optimising compiler will sacrifice compile time and the size of the produced object code for increased runtime speed. GCC in particular has 3 levels of optimisation. Some optimisations can be implemented by the programmer such as {\bf function inlining}. The {\bf inline} keyword is put in front of a function prototype and instructs the compiler to replace every call to that function with the actual code. Increasing the size of the object file but reducing the overheads associated with calling the function, returning arguments or any branching that may take place as code is kept local. Another common optimisation occurs in loops. Loop invariant code removal, where the compiler will identify statements inside a loop that may be evaluated on each iteration but may not actually change. The evaluation statement is then moved outside the loop to save on processing. Loop unrolling is similar to function inlining where multiple occurrences of the loop's code is placed in series if the loop is known to iterate for a certain number of times. This reduces the amount of times a condition will have to be checked and also how many times a {\bf goto} or {\bf jump} statement is used in the architecture's object code.
\subsection{Application of Amdahl's Law}
Unique to this implementation is that I have also produced a sequential version to act as a comparison. Linking back to section 3.2.2 - A thought process for parellism, I conducted run-time profiling of the sequential code to attempt to attain a figure for the portion of code that is improvable by parallelisation. A quick reminder:\\
\smallskip
\centerline{$ S(N) = \frac{1}{(1-P) + \frac{P}{N}} $}

A more in-depth discussion is available in section 3.2.2 and the output of GProf is available in Appendix A.9. Looking at the flat profile and the call graph, I'm assuming a value for {\bf P} to be 0.991 + 0.9932 / 2 = 0.9921. This is due to the two values reported by GProf for the percentage time spent inside the {\bf process()} function. I am using this figure because during the parallel code it is the {\bf process()} function that is made {\bf concurrent} and independent. It will be run in parallel by a certain number of threads.

The value assumed for {\bf P} means that 99\% of the code inside the sequential version of the Game of Life is eligible for improvement. There is however the concern of increasing the {\bf problem size} which happens as the parallel code is produced. This is due to the need for work scheduling and thread synchronisation. This is not taken into account whilst calculating the speed-up as I have no data to suggest its affect. The addition of these tasks will decrease the value of {\bf P} as I am adding code which is not eligible for improvement. The added code for synchronisation and work scheduling cannot be made parallel. {\it Figure 10} shows a graphical representation of the calculation of Amdahl's Law, {\it Figure 11} shows the raw data. It can be used for comparison by {\it figure 12} test data produced by the Game of Life parallel algorithm written in OpenMP.
\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{Amdahl.png}
\caption{A Graph showing the calculated speed up by application of Amdahl's Law}
\label{fig: AmdahlGraph}
\end{figure}
\subsubsection{Testing against Amdahl's Law}
A full table of results is available in the appendices and will be explained during the analysis portion of the report. For now I have included data which attempts to stimulate a discussion about the correctness of Amdahl's law. The settings for the results listed in {\it figure 12} were as follows: dimension, 1024, generations, 10000, using shared memory threads, and optimisation level 3. Referring to the introductory remarks about testing, my system is based on an Intel Q6600 quad-core processor. The greatest number of truly parallel tasks my system is capable of running is four. One thread on each core. Considering this the results are quite promising. {\it Figure 12} shows that a sequential run of the program took nearly 40 seconds to execute. Another run of the code using 2 threads showed a speed up of 1.94 which is very close to the maximum calculated speed up of 1.98. This means that the estimation for {\bf P}, the proportion of code which can be made parallel, can not have been too far off! Looking at the next run with 4 cores the result is not as pleasing with a speed up of 3.03 compared to a calculated speed up of 3.9. Still it is in line with predictions. Alas, my system does not have enough independent computational units to allow any more meaningful results to be processed and 8 threads and above the performance gets gradually worse. This is not surprising as there are significant overheads related to switching a threads state from running to blocked, and with an increased number of threads being created this case would be happening a lot more.

The maximum speed up theoretically possibly from a program where roughly 99\% of the code could be eligible for parallelism is around 126 times faster than a single threaded implementation. This is evidenced by {\it figures 10} and {\it 11} unfortunately I do not have access to a system with 65536 independent cores.

\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Speedup.png}
\caption{The Speed up by application of Amdahl's Law}
\label{fig: speedup}
\end{subfigure}
~ 
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{measured.png}
\caption{The measured speed up}
\label{fig: measured}
\end{subfigure}
\caption{Tables representing calculated and measured speed up}
\end{figure}
\subsection{Java with Threads}
Java is one of the most popular languages on the planet. It's simple object-oriented design patterns and extreme portability make it the language of choice for any type of project. The implementation of the Game of Life in this language is inherently, very different to that of a procedural language such as C. The implementation I have produced has three classes. A {\bf Grid} class which houses all of the functionality for creating, reading from, and writing to the global state arrays which are also stored within its structure. The global state arrays, in line with my research are created as Java primitive arrays of type boolean. Next a class called {\bf LifeThread} provides the functionality for actually processing the global state arrays and advancing the game to the next generation. The final class is called {\bf GoL}, this class is the controller and creates instances of the other classes and houses the main game loop. The code is available in full in the appendices A.6.

Java supports explicit threading through the use of a Thread class which is provided by the Java library. A class that wants to exploit all the abilities of a thread will have to implement or inherit the functionality of the Thread class and then ensure that an implementation is provided for a {\bf run()}  method. This method is executed by the calling class and the thread will cease to exist once run() returns. This is where the differences begin, in {\it listing 13} the main game loop is shown. There is no concept of a parallel region like in OpenMP. The main thread has to create new LifeThread objects for every single generation. Lines 9 - 10 show the application of the static work allocation algorithm. After this lines 11 and 12 show the initialisation and creation of LifeThread objects which are then stored in a vector of type LifeThread. Think of this vector as a common {\bf thread pool}. After the threads are all created and references stored inside the vector, the for loop on lines 15 - 17 runs {\bf start()} on each object. The start method implicitly calls the method {\bf run()} which had to be implemented because the class inherits from Thread. Then run will call processChunk(). A similar loop from lines 20 - 21 will now call {\bf join()} on all the threads in the vector. This means that the calling thread will have to wait until all the threads it asked to join have returned from their run methods. This synchronisation is the only type needed in the Java example and makes the code look far simpler than that of the OpenMP implementation. 
\begin{lstlisting}[language=Java, caption={Class GoL main game loop in GoL.java}]
//...
Grid g1 = new Grid(DIM, LIFE, FILENAME);
Vector<LifeThread> life_vec = new Vector<LifeThread>();

for(int gen = 0; gen < GEN; gen++)
{
    for(int x = 0; x < THREADS; x++)
    {
        int start = ((DIM / THREADS) * x) + 1;
        int stop = (DIM / THREADS) + start - 1;
        LifeThread life = new LifeThread(x, start, stop, g1, DIM);
        life_vec.add(x, life);
    }

    for(int x = 0; x < THREADS; x++)
    {
        life_vec.get(x).start();
    }   

    for(int x = 0; x < THREADS; x++)
        life_vec.get(x).join();
    
    g1.finishGen();
  
}
//...
\end{lstlisting}
This is also due to the positioning of the threaded section. Because it is inside the game loop the main thread deals with copying the ghost cells and swapping the pointers via a call to the Grid method {\bf finishGen()} shown on line 23.
%Use appendix A from The Art of Multiprocessor Programming.

%\item gcj and javac
%\item fork, join, yield, sleep
%\item Overheads? Thread creation - compare to C
%\item OOP? Threads as an object?
\subsection{C++}
The implementations presented in this section use two different libraries and two different design methods. Similar to the Java implementation, I have made use of object-orientation in C++. Both versions utilise a class called {\bf Grid}. This class provides basic functionality for the storing, and management of the global state arrays. It is created inside the main function for both the Boost and C++11 implementations.
\subsubsection{Boost}
Boost is a set of libraries which add functionality in C++ programs. Most widely used in this library are the threading features although it is based on, and provides very similar functionality to PThreads. It is a generic wrapper for threading functionality in C++ and whilst PThreads is not a native threading package for windows, code written in Boost::Threads will compile to target architecture specific threading constructs. In windows this is Windows Threads and in Unix this is PThreads. Because of this added portability with written C++ code, Boost has become a very popular library with some of the features making it in to new releases of the language.

The implementation really begins with the {\bf Thread.cpp} file. {\bf Listing} 14 is a look inside the {\bf ThreadClass} class and should look similar to the Java Thread class seen in Appendix A.6. The C++ Boost implementation can be found, in full, in the appendix A.4 and the Grid class A.5.
\begin{lstlisting}[language=c++, caption={Boost implementation Thread class methods}]
void ThreadClass::threadMain()
{
  try
  {
    processChunk();
  }
  catch (boost::thread_interrupted& interruption)
  {}
  catch (std::exception& e)
  {}

}

void ThreadClass::run()
{
	internalThread_ = boost::thread(&ThreadClass::threadMain, this);
}

void ThreadClass::join()
{
    internalThread_.join();
}
\end{lstlisting}
The big difference is the inclusion of exceptions. This is not supported in PThreads for C. This is also the first time I have used explicit threading with function delegation. This is different from both the paradigms seen so far. In OpenMP I defined a {\bf parallel region}, in Java I created a class that {\bf inherited} thread functionality. In PThreads, Boost, and C++11 threads, the creation of a thread depends on the address of an expected parameter, a callable function. In this case the {\bf threadMain()} function is passed to the boost::thread() constructor, this returns a {\bf handle} of the created thread. As soon as this is done the thread begins execution. The threadMain function then calls the {\bf processChunk()} function which uses a pointer to the Grid class to being processing the relevant cells of the global state.

It should also be noted that the {\bf main()} function for this code is extremely similar to that of the Java implementation. Performing the operations in the same manner in very similar syntax. 
\subsubsection{C++ 11 Threads}
C++11, named due to the release being approved in 2011, is unique in that it is the first time the C++ standard library has included support for threads. The newly incorporated {\bf std::thread} class provides functionality for all the standard array of creation, management, and synchronisation for threads. Even as similar as this method may be to the older Boost library, I have chosen to use a different program structure.
\begin{lstlisting}[language=c++, caption={Main game loop from C++ 11 implementation}]
//...    
vector<std::thread> threadPool;
//...
for (int gen = 0; gen < MAXGEN; gen ++)
{
    for(int x = 0; x < THREADS; x++)
    {
        int start = ((DIM / THREADS) * x) + 1;
        int stop = (DIM / THREADS) + start - 1;

        threadPool.push_back(std::thread(processChunkExtern, start, stop, grid));
    }
    
    for (auto &t : threadPool)
    {
        if(t.joinable())
            t.join();
    }
    
    grid->finishGen();
}
\end{lstlisting}
This structure focuses on the explicit delegation of functions to threads. The {\bf ThreadC11.cpp} file only has an {\bf inline void processChunkExtern()} and {\bf int main()}. Some of the contents of main are shown in {\bf listing 15}. Line 2 shows the declaration of a vector of type std::thread
\subsection{Go and Goroutines}
\subsection{Python}
%\item List comprehension
%\item Parallel processing?
%\item parallel applications of Map, Reduce, and List Comprehension? Can it be done? Reference Parallel Computing: Architectures, Algorithms, and Applications, C. Bischof et al (Eds.) P203 Implementing Data-Parallel Patterns for Shared Memory with OpenMP GOOD ARTICLE!
%\item The other paradigm? Fetching a 3x3 block of cells. Processing them, return the result... 

%\subsection{C with CUDA}

MENTION BUILD ARRAY.C!!

\section{Analysis}

% In OpenMP I defined a {\bf parallel region}, in Java I created a class that {\bf inherited} thread functionality. In PThreads, Boost, and C++11 threads, the creation of a thread depends on the address of an expected parameter, a callable function.

Head sub sections with each core value and talk about each language for at most 2 - 3 lines.

OMP - horrible to debug, evidenced by for loop producing incorrect results. symptom of implicit threading

%\subsection{Problems}
%
%%\item Should this really be here? It should really be rounded up in each subheading...
%%\item Optimisations compiler results mess up
%%\item Functional model learning curve and paradigm shift. 
%%\item Python - Using functional techniques.
%
%\subsection{Syntax and Semantics}
%\subsection{Ease of use}
%\subsection{Features}
%\subsection{Performance}
%\subsection{Extensions}

%\item Using OOP to recognize patterns and predict movement. I.e. Gliders and inverters.

\section{Conclusion}
%Some sort of choice based on the most appropriate language.
\section{Bibliography}
%Both compiler books!!
\nocite{*}
\bibliographystyle{unsrt}
\bibliography{Project.bib}
\appendix
\pagebreak
\section{Appendices}
\subsection{Sequential C Implementation}
\lstinputlisting[language=C]{../Sequential/seqGoL.c}
\pagebreak
\subsection{OpenMP C Implementation}
\lstinputlisting[language=C]{../OpenMP/edgol.c}
\pagebreak
\subsection{C++ 11 Implementation}
\lstinputlisting[language=C++]{../C++/ThreadC11.cpp}
\pagebreak
\subsection{C++ Boost Threads Implementation}
\lstinputlisting[language=C++]{../C++/Thread.cpp}
\pagebreak
\subsection{C++ Grid Class Implementation}
\lstinputlisting[language=C++]{../C++/Grid.cpp}
\pagebreak
\subsection{Java Threads Implementation}
\lstinputlisting[language=Java]{../Java/GoL.java}
\pagebreak
\subsection{Python Functional Implementation}
\lstinputlisting[language=Python]{../Pyhton/GoL.py}
\pagebreak
\subsection{Go with Go Routines Implementation}
\lstinputlisting[language=C]{../Go/GoL.go}
\pagebreak
\subsection{Flat Profile and Call-Graph for seqGoL.c using runtime profiler Gprof}
\begingroup
\fontsize{10pt}{8pt}
\begin{verbatim}
Flat profile:

Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total           
 time   seconds   seconds    calls  ms/call  ms/call  name    
 56.47      1.98     1.98 104857600     0.00     0.00  getCount
 29.95      3.02     1.05 104857600     0.00     0.00  applyRule
 12.90      3.48     0.45      100     4.51    34.76  process
  0.57      3.50     0.02        1    20.06    20.06  fillRand
  0.29      3.51     0.01        2     5.02     5.02  printGrid
  0.00      3.51     0.00      100     0.00     0.00  copyGhostCells
  0.00      3.51     0.00        1     0.00     0.00  init

 %         the percentage of the total running time of the
time       program used by this function.

cumulative a running sum of the number of seconds accounted
 seconds   for by this function and those listed above it.

 self      the number of seconds accounted for by this
seconds    function alone.  This is the major sort for this
           listing.

calls      the number of times this function was invoked, if
           this function is profiled, else blank.
 
 self      the average number of milliseconds spent in this
ms/call    function per call, if this function is profiled,
	   else blank.

 total     the average number of milliseconds spent in this
ms/call    function and its descendents per call, if this 
	   function is profiled, else blank.

name       the name of the function.  This is the minor sort
           for this listing. The index shows the location of
	   the function in the gprof listing. If the index is
	   in parenthesis it shows where it would appear in
	   the gprof listing if it were to be printed.


		     Call graph (explanation follows)
granularity: each sample hit covers 2 byte(s) for 0.29% of 3.51 seconds

index % time    self  children    called     name
                                                 <spontaneous>
[1]    100.0    0.00    3.51                 main [1]
                0.45    3.02     100/100         process [2]
                0.02    0.00       1/1           fillRand [5]
                0.01    0.00       2/2           printGrid [6]
                0.00    0.00     100/100         copyGhostCells [7]
                0.00    0.00       1/1           init [8]
-----------------------------------------------
                0.45    3.02     100/100         main [1]
[2]     99.1    0.45    3.02     100         process [2]
                1.98    0.00 104857600/104857600     getCount [3]
                1.05    0.00 104857600/104857600     applyRule [4]
-----------------------------------------------
                1.98    0.00 104857600/104857600     process [2]
[3]     56.4    1.98    0.00 104857600         getCount [3]
-----------------------------------------------
                1.05    0.00 104857600/104857600     process [2]
[4]     29.9    1.05    0.00 104857600         applyRule [4]
-----------------------------------------------
                0.02    0.00       1/1           main [1]
[5]      0.6    0.02    0.00       1         fillRand [5]
-----------------------------------------------
                0.01    0.00       2/2           main [1]
[6]      0.3    0.01    0.00       2         printGrid [6]
-----------------------------------------------
                0.00    0.00     100/100         main [1]
[7]      0.0    0.00    0.00     100         copyGhostCells [7]
-----------------------------------------------
                0.00    0.00       1/1           main [1]
[8]      0.0    0.00    0.00       1         init [8]
-----------------------------------------------

Index by function name

   [4] applyRule               [3] getCount                [2] process
   [7] copyGhostCells          [8] init
   [5] fillRand                [6] printGrid

\end{verbatim}
\endgroup
\pagebreak
\subsection{Valgrind output}
\includegraphics[scale=0.5]{valgrind1.png}\\
\includegraphics[scale=0.5]{valgrind2.png}
\pagebreak
\subsection{Bool and Int research with C file and ASM file}
\lstinputlisting[language=C]{../Sequential/boolInt.c}
\lstinputlisting[language={[x86masm]Assembler}]
{../Sequential/boolInt.s}
\pagebreak
\end{document}